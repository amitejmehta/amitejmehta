{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This cell is only necessary if using python environments with jupyter notebooks. Prevents having to restart the kernel in order for changes made to modules imported to be reflected*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import inference\n",
    "importlib.reload(inference)\n",
    "\n",
    "# Now, you can use the updated 'your_function_name' or other functions from 'generate'\n",
    "from inference import generate_token_w_caching, get_top_k, generate, generate_no_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/Users/amitej/amitejmehta/models/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1544,  373,  262, 6478,  286, 4842,  287,  262, 1160,   82,  290,  465,\n",
       "         1438,  373,  978]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"He was the boss of Chicago in the 20s and his name was Al\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the boss of Chicago in the 20s and his name was Al Gore\n"
     ]
    }
   ],
   "source": [
    "next_token_id, past_key_values = generate_token_w_caching(inputs, model)\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(f'{prompt}{next_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most likely tokens: [' Gore', ' Cap', '.', ' Pac', ' Shar', 'on', ' Smith', 'ton', '-', ' Franken']\n"
     ]
    }
   ],
   "source": [
    "top_k = get_top_k(inputs, model, tokenizer, k=10)\n",
    "print(f\"Top 10 most likely tokens: {top_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model believes the most likely token to follow Al is Gore, despite the context which suggests we are talking about Al Capone. While this error is in large part due to GPT-2s shortcomings, we do see that the second most likely token is ' Cap' the first of two tokens that make up 'Capone'. We'll explore later how different sampling techniques can improve the reponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the boss of Chicago in the 20s and his name was Al Gore. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the\n",
      "Generated 50 tokens in 1.897346019744873s\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 50\n",
    "t0 = time.time()\n",
    "generated_tokens = generate_no_caching(inputs, model, tokenizer, max_tokens)\n",
    "t1 = time.time() - t0\n",
    "print(f'{prompt}{\"\".join(generated_tokens)}')\n",
    "print(f'Generated {max_tokens} tokens in {t1}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the boss of Chicago in the 20s and his name was Al Gore. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the\n",
      "Generated 50 tokens in 0.9406602382659912s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "generated_tokens = generate(inputs, model, tokenizer, max_tokens)\n",
    "t1 = time.time() - t0\n",
    "print(f'{prompt}{\"\".join(generated_tokens)}')\n",
    "print(f'Generated {max_tokens} tokens in {t1}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 50 tokens, generation without caching takes twice as long as generation with. The longer the output the more time caching saves us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
