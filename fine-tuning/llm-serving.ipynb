{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This cell is only necessary if using python environments with jupyter notebooks. Prevents having to restart the kernel in order for changes made to modules imported to be reflected*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import inference\n",
    "importlib.reload(inference)\n",
    "\n",
    "\n",
    "# Now, you can use the updated 'your_function_name' or other functions from 'generate'\n",
    "from inference import generate_token_w_caching, get_top_k, generate, generate_no_caching, generate_one_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/Users/amitej/amitejmehta/models/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1544,  373,  262, 6478,  286, 4842,  287,  262, 1160,   82,  290,  465,\n",
       "         1438,  373,  978]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\"He was the boss of Chicago in the 20s and his name was Al\"]\n",
    "input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the boss of Chicago in the 20s and his name was Al Gore\n"
     ]
    }
   ],
   "source": [
    "next_token_id, past_key_values = generate_token_w_caching(input, model)\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(f'{prompt}{next_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most likely tokens: [' Gore', ' Cap', '.', ' Pac', ' Shar', 'on', ' Smith', 'ton', '-', ' Franken']\n"
     ]
    }
   ],
   "source": [
    "top_k = get_top_k(input, model, tokenizer, k=10)\n",
    "print(f\"Top 10 most likely tokens: {top_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model believes the most likely token to follow Al is Gore, despite the context which suggests we are talking about Al Capone. While this error is in large part due to GPT-2s shortcomings, we do see that the second most likely token is ' Cap' the first of two tokens that make up 'Capone'. We'll explore later how different sampling techniques can improve the reponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the boss of Chicago in the 20s and his name was Al Gore. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the\n",
      "Generated 50 tokens in 1.897346019744873s\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 50\n",
    "t0 = time.time()\n",
    "generated_tokens = generate_no_caching(input, model, tokenizer, max_tokens)\n",
    "t1 = time.time() - t0\n",
    "print(f'{prompt}{\"\".join(generated_tokens)}')\n",
    "print(f'Generated {max_tokens} tokens in {t1}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the boss of Chicago in the 20s and his name was Al Gore. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the\n",
      "Generated 50 tokens in 0.9406602382659912s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "generated_tokens = generate_one_sequence(input, model, tokenizer, max_tokens)\n",
    "t1 = time.time() - t0\n",
    "print(f'{prompt}{\"\".join(generated_tokens)}')\n",
    "print(f'Generated {max_tokens} tokens in {t1}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 50 tokens, generation without caching takes twice as long as generation with. The longer the output the more time caching saves us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256,  1544,   373,   262,  6478,   286,\n",
       "          4842,   287,   262,  1160,    82,   290,   465,  1438,   373,   978],\n",
       "        [  818,  3776,   286, 20902,    11, 35754, 20956,   550,  1936,  1751,\n",
       "            13,  3574, 13325,   284, 18887,    11,   511,  3891,   547,   220],\n",
       "        [50256, 50256, 50256,  4299,  7716,    62, 30001,     7, 15414,    82,\n",
       "            11,  2746,    11, 19232,  2625, 16694,  4716,     1,  2599,   220]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\"He was the boss of Chicago in the 20s and his name was Al\",\n",
    "           \"In Game of Thrones, Ned Stark had five children. From oldest to youngest, their names were \",\n",
    "           'def generate_token(inputs, model, sampling=\"greedy\"): ']\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "He was the boss of Chicago in the 20s and his name was Al in\n",
      "In Game of Thrones, Ned Stark had five children. From oldest to youngest, their names were  \n",
      "def generate_token(inputs, model, sampling=\"greedy\"):  \n"
     ]
    }
   ],
   "source": [
    "next_token_ids, past_key_values = generate_token_w_caching(inputs, model)\n",
    "\n",
    "print(next_token_ids.shape)\n",
    "next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f'{prompt}{next_tokens[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the boss of Chicago in the 20s and his name was Al \"\u001b[31m Gore. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States. He was the president of the United States\u001b[0m\n",
      "\n",
      "In Game of Thrones, Ned Stark had five children. From oldest to youngest, their names were  \"\u001b[31m Darth, Dany, Dany, and Dany.\n",
      "Dany was the youngest, and Dany was the oldest.\n",
      "Dany was the youngest, and Dany was the oldest.\n",
      "Dany was the youngest, and Dany was the oldest.\n",
      "Dany was the youngest, and Dany was the oldest.\n",
      "Dany was the youngest, and Dany was the oldest.\n",
      "Dany was the youngest, and Dany was the oldest.\n",
      "D\u001b[0m\n",
      "\n",
      "def generate_token(inputs, model, sampling=\"greedy\"):  \"\u001b[31m return  {\n",
      "\n",
      "\"inputs\": {\n",
      "\n",
      "\"model\": \"greedy\",\n",
      "\n",
      "\"sample\": \"greedy\",\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "# This is the output of the generator\n",
      "\n",
      "#\n",
      "\n",
      "# The output of the generator is a list of all the inputs\n",
      "\n",
      "#\n",
      "\n",
      "# The output of the generator is a list of all the inputs\n",
      "\n",
      "#\n",
      "\n",
      "# The output of the generator is a\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = generate(inputs, model, tokenizer, 100)\n",
    "\n",
    "for prompt, generated in zip(prompts, generated_texts):\n",
    "    print(prompt, f'\"\\x1b[31m{generated}\\x1b[0m\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
