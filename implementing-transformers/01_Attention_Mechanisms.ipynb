{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded vector: tensor([[ 0.3374, -0.1778, -0.3035],\n",
      "        [ 0.1794,  1.8951,  0.4954],\n",
      "        [ 0.2692, -0.0770, -1.0205],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-0.5880,  0.3486,  0.6603],\n",
      "        [-1.1925,  0.6984, -1.4097]])\n",
      "Shape of embeddeding torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"Life is short eat dessert first\"\n",
    "\n",
    "dct = {word:i for i, word in enumerate(sorted(sentence.split()))}\n",
    "sentence_int = torch.tensor([dct[s] for s in sentence.split()])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(vocab_size, 3)\n",
    "\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(f\"Embedded vector: {embedded_sentence}\")\n",
    "print(f'Shape of embeddeding {embedded_sentence.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 2,2,4\n",
    "\n",
    "W_q = torch.nn.Parameter(torch.rand(d, d_q))\n",
    "W_k = torch.nn.Parameter(torch.rand(d, d_k))\n",
    "W_v = torch.nn.Parameter(torch.rand(d, d_v))\n",
    "\n",
    "\n",
    "print(W_q.shape)\n",
    "print(W_k.shape)\n",
    "print(W_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second token query vector: tensor([0.5667, 1.8269], grad_fn=<SqueezeBackward4>) . Shape: torch.Size([2])\n",
      "Second token key vector: tensor([0.5295, 1.7355], grad_fn=<SqueezeBackward4>) . Shape: torch.Size([2])\n",
      "Second token value vector: tensor([0.6612, 1.8972, 1.0963, 1.8106], grad_fn=<SqueezeBackward4>) . Shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "q_2 = x_2 @ W_q\n",
    "k_2 = x_2 @ W_k\n",
    "v_2 = x_2 @ W_v\n",
    "\n",
    "print(f\"Second token query vector: {q_2} . Shape: {q_2.shape}\")\n",
    "print(f\"Second token key vector: {k_2} . Shape: {k_2.shape}\")\n",
    "print(f\"Second token value vector: {v_2} . Shape: {v_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys : tensor([[-0.0823, -0.3031],\n",
      "        [ 0.5295,  1.7355],\n",
      "        [-0.2991, -0.7295],\n",
      "        [ 0.1420,  0.2291],\n",
      "        [ 0.1920,  0.6467],\n",
      "        [-0.4788, -0.5835]], grad_fn=<MmBackward0>) Shape of Keys : torch.Size([6, 2])\n",
      "Values : tensor([[-0.2546, -0.2608, -0.1544, -0.2801],\n",
      "        [ 0.6612,  1.8972,  1.0963,  1.8106],\n",
      "        [-0.8598, -0.6161, -0.5940, -0.9455],\n",
      "        [ 0.5932,  0.0981,  0.2741,  0.4151],\n",
      "        [ 0.5605,  0.5645,  0.3676,  0.6429],\n",
      "        [-1.2107, -0.4929, -1.0081, -1.4031]], grad_fn=<MmBackward0>) Shape of values : torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "keys = embedded_sentence @ W_k\n",
    "values = embedded_sentence @ W_v\n",
    "\n",
    "print(f\"Keys : {keys} Shape of Keys : {keys.shape}\")\n",
    "print(f\"Values : {values} Shape of values : {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2903, grad_fn=<DotBackward0>)\n",
      "Unormalized attention weights of each word to the second input token tensor([-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = q_2.dot(keys[4])\n",
    "print(omega_24)\n",
    "\n",
    "omega_2 = q_2 @ keys.T\n",
    "print(f\"Unormalized attention weights of each word to the second input token {omega_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized attention weights with respect to second token tensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "alpha_2 = F.softmax(omega_2/d_k**0.5, dim=-1)\n",
    "\n",
    "print(f\"Normalized attention weights with respect to second token {alpha_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5313, 1.3607, 0.7891, 1.3110], grad_fn=<SqueezeBackward4>)\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = alpha_2 @ values\n",
    "\n",
    "\n",
    "print(context_vector_2)\n",
    "print(context_vector_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_k = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_q = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_v = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "        queries = x @ self.W_q\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / self.d_out_kq**0.5, dim=-1)\n",
    "        context = attn_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2358,  0.0274, -0.1529, -0.1919],\n",
      "        [ 0.5449,  1.4054,  0.8220,  1.3609],\n",
      "        [-0.4417, -0.1620, -0.3432, -0.4809],\n",
      "        [ 0.0351,  0.3622,  0.1272,  0.2443],\n",
      "        [ 0.2236,  0.6611,  0.3469,  0.5929],\n",
      "        [-0.3897, -0.1226, -0.2999, -0.4153]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa = SelfAttention(3, 2, 4)\n",
    "print(sa(embedded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttention(d_in, d_out_kq, d_out_v) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0529],\n",
       "        [ 0.4134],\n",
       "        [-0.1403],\n",
       "        [ 0.0794],\n",
       "        [ 0.1848],\n",
       "        [-0.1201]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d_in, d_out_kq, d_out_v = 3,2,1\n",
    "\n",
    "sa = SelfAttention(d_in, d_out_kq, d_out_v)\n",
    "\n",
    "sa(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0529,  0.1157,  0.1206,  0.0557],\n",
       "        [ 0.4134,  1.6489,  1.4377,  0.9843],\n",
       "        [-0.1403, -0.0389, -0.0751,  0.0052],\n",
       "        [ 0.0794,  0.1602,  0.3335, -0.1488],\n",
       "        [ 0.1848,  0.3579,  0.5004, -0.0543],\n",
       "        [-0.1201, -0.1457, -0.1849, -0.2312]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out_kq, d_out_v, num_heads=4)\n",
    "\n",
    "mha(embedded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_k = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_v = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "        self.d_out_kq = d_out_kq\n",
    "    \n",
    "    def forward(self, x_1, x_2):\n",
    "        queries_1 = x_1 @ self.W_q\n",
    "        keys_2 = x_2 @ self.W_k\n",
    "        values_2 = x_2 @ self.W_v\n",
    "\n",
    "        attn_scores = queries_1 @ keys_2.T\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores/self.d_out_kq**0.5, dim=-1)\n",
    "\n",
    "        context_vectors= attn_weights @ values_2\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input shape torch.Size([6, 3])\n",
      "Second input shape torch.Size([8, 3])\n",
      "tensor([[0.4231, 0.8665, 0.6503, 1.0042],\n",
      "        [0.4874, 0.9718, 0.7359, 1.1353],\n",
      "        [0.4054, 0.8359, 0.6258, 0.9667],\n",
      "        [0.4357, 0.8886, 0.6678, 1.0311],\n",
      "        [0.4429, 0.9006, 0.6775, 1.0460],\n",
      "        [0.3860, 0.8021, 0.5985, 0.9250]], grad_fn=<MmBackward0>)\n",
      "Output shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d_in, d_out_kq, d_out_v = 3,2,4\n",
    "\n",
    "crossattn = CrossAttention(d_in, d_out_kq, d_out_v)\n",
    "\n",
    "first_input = embedded_sentence\n",
    "second_input = torch.rand(8, d_in)\n",
    "\n",
    "print(\"First input shape\", first_input.shape)\n",
    "print(\"Second input shape\", second_input.shape)\n",
    "\n",
    "context_vectors = crossattn(first_input, second_input)\n",
    "print(context_vectors)\n",
    "print(\"Output shape:\", context_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, mask=None):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_k = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_v = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_in = d_in\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_q\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "        print(queries.shape)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        block_size = attn_scores.shape[0]\n",
    "        if self.mask:\n",
    "            block_size = attn_scores.shape[0]\n",
    "            mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "            attn_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores/self.d_out_kq**0.5, dim=-1)\n",
    "        print(attn_weights)\n",
    "\n",
    "        context_vector = attn_weights @ values\n",
    "\n",
    "        return context_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],\n",
      "        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],\n",
      "        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1055],\n",
       "        [ 0.5085],\n",
       "        [-0.1312],\n",
       "        [ 0.1236],\n",
       "        [ 0.1905],\n",
       "        [-0.1827]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "msa = MaskedSelfAttention(3,2,1)\n",
    "\n",
    "msa(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedSelfAttention(d_in, d_out_kq, d_out_v) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],\n",
      "        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],\n",
      "        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0.1055],\n",
      "        [ 0.5085],\n",
      "        [-0.1312],\n",
      "        [ 0.1236],\n",
      "        [ 0.1905],\n",
      "        [-0.1827]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "mmha = MaskedMultiHeadAttentionWrapper(3, 2, 1, 1)\n",
    "\n",
    "print(mmha(embedded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a KV-Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on we will assume we are working with a decoder only language model. Therefore, we won't have cross-attention and we will use a mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, max_batch_size, max_seq_len, d_in, d_out_kq, d_out_v, num_heads, kv_cache=False):\n",
    "        self.cache_k = torch.zeros((max_batch_size, max_seq_len, num_heads, d_out_kq))\n",
    "        self.cache_v = torch.zeros((max_batch_size, max_seq_len, num_heads, d_out_v))\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
