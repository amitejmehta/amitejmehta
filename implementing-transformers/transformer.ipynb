{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int = 512\n",
    "    d_ff: int = 2048\n",
    "    N: int=6\n",
    "    dropout: float =0.1\n",
    "    num_heads: int = 8\n",
    "    vocab_size: int = 50000\n",
    "    pad_idx : int = 0\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
    "        self.d_model = args.d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)*self.d_model**0.5\n",
    "\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, attn_mask=False, pad_mask=True):\n",
    "        super().__init__()\n",
    "        self.attn_mask=attn_mask\n",
    "        self.pad_mask=pad_mask\n",
    "        self.heads = nn.ModuleList([SelfAttention(args, self.attn_mask, self.pad_mask) for _ in range(args.num_heads)])\n",
    "    \n",
    "    #x_2 for cross attention\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        if encoder_output:\n",
    "            return torch.cat([head(x, encoder_output) for head in self.heads], dim=-1)\n",
    "        else:\n",
    "            return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, attn_mask=False, pad_mask=True):\n",
    "        super().__init__()\n",
    "        d_in = args.d_model\n",
    "        self.d_out_kq = d_in // args.d_model\n",
    "        d_out_v = self.d_out_kq\n",
    "        self.W_q = nn.Parameter(torch.rand(d_in, self.d_out_kq))\n",
    "        self.W_k = nn.Parameter(torch.rand(d_in, self.d_out_kq))\n",
    "        self.W_v = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "        self.attn_mask = attn_mask\n",
    "        self.pad_mask=pad_mask\n",
    "        self.pad_idx = args.pad_idx\n",
    "\n",
    "    #x_2 for cross attention\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        if encoder_output:\n",
    "            queries = x @ self.W_q\n",
    "            keys = encoder_output @ self.W_k\n",
    "            values = encoder_output @ self.W_v\n",
    "        else:\n",
    "            queries = x @ self.W_q\n",
    "            keys = x @ self.W_k\n",
    "            values = x @ self.W_v\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        #for masked attention\n",
    "        if self.attn_mask:\n",
    "            block_size = attn_scores.shape[0]\n",
    "            mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "            attn_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        \n",
    "        if self.pad_mask:\n",
    "            if encoder_output is not None:\n",
    "                pad_mask = (encoder_output == self.pad_idx).all(dim=-1)  # Adjusted for encoder_output\n",
    "            else:\n",
    "                pad_mask = (x == self.pad_idx).all(dim=-1)  # Original input mask\n",
    "            pad_mask = pad_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_scores = attn_scores.masked_fill(pad_mask, float('-inf'))\n",
    "                #calculate and apply pad_mask\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores/self.d_out_kq**0.5, dim=-1)\n",
    "        context_vector = attn_weights @ values\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in_out, d_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_in_out, d_hidden)\n",
    "        self.w2 = nn.Linear(d_hidden, d_in_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.ReLU(self.w1(x))\n",
    "        return self.w2(self.dropout(x))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.ff = PositionWiseFeedForward(args.d_model, args.d_ff, args.dropout)\n",
    "        self.mha = MultiHeadAttention(args)\n",
    "        self.norm1 = nn.LayerNorm(args.d_model)\n",
    "        self.norm2 = nn.LayerNorm(args.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.mha(x))\n",
    "        return self.norm2(x+self.ff(x))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(args) for _ in range(args.N)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.mmha = MultiHeadAttention(args, attn_mask=True)\n",
    "        self.mhca = MultiHeadAttention(args)\n",
    "        self.ff = PositionWiseFeedForward(args.d_model, args.d_ff, args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.d_model)\n",
    "        self.norm2 = nn.LayerNorm(args.d_model)\n",
    "        self.norm3 = nn.LayerNorm(args.d_model)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        x = self.norm1(x+self.mmha(x))\n",
    "        x = self.norm2(x + self.mhca(x, encoder_output))\n",
    "        return self.norm3(x + self.ff(x))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(args) for _ in range(args.N)])\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output)\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.input_embedding = Embeddings(args)\n",
    "        self.output_embedding = Embeddings(args)\n",
    "        self.encoder = Encoder(args)\n",
    "        self.decoder = Decoder(args)\n",
    "        self.linear = nn.Linear(args.d_model, args.vocab_size)\n",
    "        self.linear.weight = self.input_embedding.embedding.weight\n",
    "        self.output_embedding.weight = self.input_embedding.embedding.weight\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.encoder(self.input_embedding(src))\n",
    "        tgt = self.shift_right(tgt)\n",
    "        tgt = self.decoder(self.output_embedding(tgt), src)\n",
    "        return torch.log_softmax(self.linear(x), dim=-1)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (input_embedding): Embeddings(\n",
      "    (embedding): Embedding(50000, 512)\n",
      "  )\n",
      "  (output_embedding): Embeddings(\n",
      "    (embedding): Embedding(50000, 512)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=50000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args = ModelArgs()\n",
    "print(Transformer(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q torch.Size([512, 512])\n",
      "W_k torch.Size([512, 512])\n",
      "W_v torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "for name, param in sa.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
