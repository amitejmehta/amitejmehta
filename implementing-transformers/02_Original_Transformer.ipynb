{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())\n",
    "else:\n",
    "    print(\"No GPU, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For code simplicity, we can define some model parameters for use within our components. The default values are those of the original transformer (base size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int = 512 #size of the embedding vectors\n",
    "    d_ff: int = 2048 #size of the hidden dimensions of the feed forward networks\n",
    "    N: int=6 #the number of encoder and decoder layers\n",
    "    dropout: float = 0.1 \n",
    "    num_heads: int = 8 #the number of heads in each attention block\n",
    "    vocab_size: int = 50000 #the size of the vocabulary\n",
    "    pad_idx : int = 0 #index of the padding token \n",
    "    max_seq_len: int = 5000\n",
    "\n",
    "model_args=ModelArgs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Unlike recurrent neural networks or convolutional neural networks, whose mechanisms naturally understand position, attention does not. Thus, some kind of positional information must be included to make sure \"man evolved from apes\" is not treated the same as \"apes evolved from man.\" \n",
    "\n",
    "These are added to the input embeddings, thus we know that we must create positional encoding vectors of `d_model`. \n",
    "\n",
    "Let's break down the equations piece by piece.\n",
    "\n",
    "P_E(pos, 2i) = sin(pos / 10000^(2i / d_model))\n",
    "\n",
    "P_E(pos, 2i+1) = cos(pos / 10000^(2i / d_model))\n",
    "\n",
    "`pos` is the current position we are focusing on. There can be up to `max_seq_len` positions in given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.d_model = args.d_model\n",
    "        self.max_length = args.max_seq_len\n",
    "        self.encoding = torch.zeros(self.max_length, self.d_model)\n",
    "\n",
    "\n",
    "        self.pos = torch.arange(0, self.max_length).unsqueeze(1)\n",
    "\n",
    "        self.denominator = torch.exp(torch.arange(0, self.d_model, 2) * -(math.log(10000.0) / self.d_model))\n",
    "\n",
    "        self.quotient = self.pos * self.denominator\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(self.quotient)\n",
    "        self.encoding[:, 1::2] = torch.cos(self.quotient)\n",
    "\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, : x.size(1)].detach()\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length:  5000\n",
      "Size of embedding vector: 512\n",
      "Positions:  torch.Size([5000, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([5000, 256])\n",
      "torch.Size([1, 5000, 512])\n"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoding(model_args)\n",
    "print(\"Max Sequence Length: \", model_args.max_seq_len)\n",
    "print(\"Size of embedding vector:\", model_args.d_model)\n",
    "print(\"Positions: \", pe.pos.shape)\n",
    "print(pe.denominator.shape)\n",
    "print(pe.quotient.shape)\n",
    "print(pe.encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
    "        self.d_model = args.d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)*self.d_model**0.5\n",
    "\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, attn_mask=False, pad_mask=True):\n",
    "        super().__init__()\n",
    "        self.attn_mask=attn_mask\n",
    "        self.pad_mask=pad_mask\n",
    "        self.heads = nn.ModuleList([SelfAttention(args, self.attn_mask, self.pad_mask) for _ in range(args.num_heads)])\n",
    "    \n",
    "    #x_2 for cross attention\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        if encoder_output:\n",
    "            return torch.cat([head(x, encoder_output) for head in self.heads], dim=-1)\n",
    "        else:\n",
    "            return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, attn_mask=False, pad_mask=True):\n",
    "        super().__init__()\n",
    "        d_in = args.d_model\n",
    "        self.d_out_kq = d_in // args.d_model\n",
    "        d_out_v = self.d_out_kq\n",
    "        self.W_q = nn.Parameter(torch.rand(d_in, self.d_out_kq))\n",
    "        self.W_k = nn.Parameter(torch.rand(d_in, self.d_out_kq))\n",
    "        self.W_v = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "        self.attn_mask = attn_mask\n",
    "        self.pad_mask=pad_mask\n",
    "        self.pad_idx = args.pad_idx\n",
    "\n",
    "    #x_2 for cross attention\n",
    "    def forward(self, x, encoder_output=None):\n",
    "        if encoder_output is not None:\n",
    "            queries = x @ self.W_q\n",
    "            keys = encoder_output @ self.W_k\n",
    "            values = encoder_output @ self.W_v\n",
    "        else:\n",
    "            queries = x @ self.W_q\n",
    "            keys = x @ self.W_k\n",
    "            values = x @ self.W_v\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        #for masked attention\n",
    "        if self.attn_mask:\n",
    "            block_size = attn_scores.shape[0]\n",
    "            mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "            attn_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        \n",
    "        if self.pad_mask:\n",
    "            if encoder_output is not None:\n",
    "                pad_mask = (encoder_output == self.pad_idx).all(dim=-1)  # Adjusted for encoder_output\n",
    "            else:\n",
    "                pad_mask = (x == self.pad_idx).all(dim=-1)  # Original input mask\n",
    "            pad_mask = pad_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_scores = attn_scores.masked_fill(pad_mask, float('-inf'))\n",
    "                #calculate and apply pad_mask\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores/self.d_out_kq**0.5, dim=-1)\n",
    "        context_vector = attn_weights @ values\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in_out, d_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_in_out, d_hidden)\n",
    "        self.w2 = nn.Linear(d_hidden, d_in_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.ReLU(self.w1(x))\n",
    "        return self.w2(self.dropout(x))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.ff = PositionWiseFeedForward(args.d_model, args.d_ff, args.dropout)\n",
    "        self.mha = MultiHeadAttention(args)\n",
    "        self.norm1 = nn.LayerNorm(args.d_model)\n",
    "        self.norm2 = nn.LayerNorm(args.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.mha(x))\n",
    "        return self.norm2(x+self.ff(x))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(args) for _ in range(args.N)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.mmha = MultiHeadAttention(args, attn_mask=True)\n",
    "        self.mhca = MultiHeadAttention(args)\n",
    "        self.ff = PositionWiseFeedForward(args.d_model, args.d_ff, args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.d_model)\n",
    "        self.norm2 = nn.LayerNorm(args.d_model)\n",
    "        self.norm3 = nn.LayerNorm(args.d_model)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        x = self.norm1(x+self.mmha(x))\n",
    "        x = self.norm2(x + self.mhca(x, encoder_output))\n",
    "        return self.norm3(x + self.ff(x))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(args) for _ in range(args.N)])\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output)\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.input_embedding = Embeddings(args)\n",
    "        self.output_embedding = Embeddings(args)\n",
    "        self.positional_encoding = (PositionalEncoding(args))\n",
    "        self.encoder = Encoder(args)\n",
    "        self.decoder = Decoder(args)\n",
    "        self.linear = nn.Linear(args.d_model, args.vocab_size)\n",
    "        self.linear.weight = self.input_embedding.embedding.weight\n",
    "        self.output_embedding.weight = self.input_embedding.embedding.weight\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src = torch.cat(self.positional_encoding(src), self.input_embedding(src), dim=0)\n",
    "        src = self.encoder(self.input_embedding(src))\n",
    "        tgt = self.shift_right(tgt)\n",
    "        tgt = self.decoder(self.output_embedding(tgt), src)\n",
    "        return torch.log_softmax(self.linear(src), dim=-1)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (input_embedding): Embeddings(\n",
      "    (embedding): Embedding(50000, 512)\n",
      "  )\n",
      "  (output_embedding): Embeddings(\n",
      "    (embedding): Embedding(50000, 512)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (mmha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (mhca): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0): SelfAttention()\n",
      "            (1): SelfAttention()\n",
      "            (2): SelfAttention()\n",
      "            (3): SelfAttention()\n",
      "            (4): SelfAttention()\n",
      "            (5): SelfAttention()\n",
      "            (6): SelfAttention()\n",
      "            (7): SelfAttention()\n",
      "          )\n",
      "        )\n",
      "        (ff): PositionWiseFeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=50000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args = ModelArgs()\n",
    "print(Transformer(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q torch.Size([512, 512])\n",
      "W_k torch.Size([512, 512])\n",
      "W_v torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "for name, param in sa.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
