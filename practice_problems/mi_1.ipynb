{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Coding Questions (Python + PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to create a PyTorch tensor of shape (5, 3) filled with random floats between 0 and 1.\n",
    "Demonstrate how to perform element-wise addition, multiplication, and matrix multiplication between two tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Building and Training a Simple Model\n",
    "\n",
    "Implement a simple feedforward neural network in PyTorch that could, in theory, be used for sequence classification (e.g., classifying protein sequences into functional categories).\n",
    "Include a training loop with dummy data and explain each step of the process, including loss calculation and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.f1 = nn.Linear(embed_dim, n_hidden)\n",
    "        self.f2 = nn.Linear(n_hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        x=self.emb(x)\n",
    "        x = self.f1(x)\n",
    "        x = F.relu(x) #we choose ReLu because\n",
    "        x = self.f2(x)\n",
    "        return x\n",
    "\n",
    "#How do you choose the correct number of hidden units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleFeedForward(\n",
      "  (emb): Embedding(50000, 512)\n",
      "  (f1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (f2): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n",
      "Features: torch.Size([256, 6])\n",
      "Labels: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "#generate dummy data\n",
    "vocab_size = 50000\n",
    "embed_dim = 512\n",
    "num_examples = 256\n",
    "sequence_length = 6\n",
    "n_hidden = 256\n",
    "num_classes = 15\n",
    "\n",
    "sff = SimpleFeedForward(vocab_size, embed_dim, n_hidden, num_classes)\n",
    "print(sff)\n",
    "\n",
    "torch.manual_seed(89)\n",
    "features = torch.randint(0, vocab_size+1, (num_examples, sequence_length))\n",
    "\n",
    "labels = torch.randint(0, num_classes, (num_examples,))\n",
    "\n",
    "print(f'Features: {features.shape}')\n",
    "print(f'Labels: {labels.shape}')\n",
    "\n",
    "dataset = TensorDataset(features, labels)\n",
    "train_loader = DataLoader(dataset, batch_size = 32, shuffle=True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m sff(inputs)\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m, labels)\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(sff.parameters(), lr= 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "training_loss=0\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs, labels = batch\n",
    "    optimizer.zero_grad()\n",
    "    outputs = sff(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    training_loss+=loss.item()\n",
    "    print(training_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Sequential Data\n",
    "\n",
    "Given a sequence of amino acids (represented as a one-hot encoded tensor), write a PyTorch module that uses an LSTM layer to process the sequence and output a classification.\n",
    "Explain your choice of loss function and the significance of the LSTM's hidden state.\n",
    "Implementing a Custom Loss Function\n",
    "\n",
    "In the context of protein sequence generation, design a custom loss function in PyTorch that penalizes deviations from a target protein structure while encouraging novel therapeutics properties. Provide a high-level explanation of how you would implement and integrate this loss function into a training loop.\n",
    "Diffusion Models for Protein Generation\n",
    "\n",
    "Provide a brief overview of how you would implement a basic diffusion model in PyTorch for generating protein sequences. Discuss the key components of the model and how they work together to generate new sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate Dummy Data for the Next Token Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: torch.Size([1000, 19])\n",
      "Targets: torch.Size([1000, 19])\n",
      "Training Set: 700\n",
      "Testing Set: 300\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "batch_size = 32\n",
    "num_examples = 1000\n",
    "sequence_length = 20\n",
    "\n",
    "data = torch.randint(0, vocab_size+1, (num_examples, sequence_length))\n",
    "features = data[:, :-1]\n",
    "targets = data[:, 1:]\n",
    "\n",
    "print(f'Features: {features.shape}')\n",
    "print(f'Targets: {targets.shape}')\n",
    "\n",
    "dataset = TensorDataset(features, targets)\n",
    "train, test = random_split(dataset, lengths = [0.7, 0.3])\n",
    "\n",
    "print(f\"Training Set: {len(train)}\")\n",
    "print(f\"Testing Set: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generate a Simple Feed Forward Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFFBC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.f2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.f2(x) #no softmax because assuming Cross-Entropy Loss Later On\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFBC(\n",
      "  (f1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (f2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = 512\n",
    "sffbc = SFFBC(input_dim, 256)\n",
    "print(sffbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. #### Generate dummy data and train the simple net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: torch.Size([1000, 512])\n",
      "Labels: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "num_examples = 1000\n",
    "features = torch.randn(1000, 512)\n",
    "labels = torch.randint(0, 2, (num_examples,))\n",
    "\n",
    "print(f'Features: {features.shape}')\n",
    "print(f'Labels: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = random_split(TensorDataset(features, labels), lengths=[0.7,0.3])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch 0 Training Loss: 0.007162179797887802\n",
      "Epoch: 0 Batch 5 Training Loss: 0.12291115000844002\n",
      "Epoch: 0 Batch 10 Training Loss: 0.3158949481556192\n",
      "Epoch: 0 Batch 15 Training Loss: 0.5706950673134997\n",
      "Epoch: 0 Batch 20 Training Loss: 0.7860884377034381\n",
      "Epoch: 1 Batch 0 Training Loss: 0.8025870588840917\n",
      "Epoch: 1 Batch 5 Training Loss: 0.8381681521233986\n",
      "Epoch: 1 Batch 10 Training Loss: 1.054530896693177\n",
      "Epoch: 1 Batch 15 Training Loss: 1.5023427549560437\n",
      "Epoch: 1 Batch 20 Training Loss: 1.8411152269502054\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(sffbc.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "training_loss = 0\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    sffbc.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        outputs = sffbc(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "        if (i % 5 == 0):\n",
    "            print(f'Epoch: {epoch} Batch {i} Training Loss: {training_loss/5}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Write the Equations for and Implement the Self Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_out):\n",
    "        super().__init__()\n",
    "        self.W_v = nn.Linear(d_model, d_out)\n",
    "        self.W_q = nn.Linear(d_model, d_out)\n",
    "        self.W_k = nn.Linear(d_model, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "                               \n",
    "        attention_scores = queries @ keys.transpose(-2,-1)\n",
    "        attention_scores = attention_scores/self.d_model**0.5\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        context_vector = attention_weights @ values\n",
    "\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create an Embedding Layer and LayerNorm Module and Apply the LayerNorm to the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: torch.Size([16, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "emb_dim = 512\n",
    "vocab_size = 50000\n",
    "seq_len = 20\n",
    "data = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,vocab_size, num_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, num_dim)\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)**self.num_dim**0.5\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "e = Embeddings(vocab_size, emb_dim)\n",
    "es = e(data)\n",
    "print(f\"Embeddings Shape: {es.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # standardizes across features for each data point\n",
    "        #standarization operation (mean/stdev) batched with vectorization\n",
    "        #[batch size, seq_length, d_model]\n",
    "        #compute over dimension last dimension which is of size d_model\n",
    "        mean = torch.mean(x, dim=-1, keepdim = True)\n",
    "        stdev = torch.std(x, dim=-1, keepdim=True)\n",
    "        x = (x-mean) / stdev\n",
    "        return x\n",
    "\n",
    "ln = LayerNorm()\n",
    "esln = ln(es)\n",
    "\n",
    "print(esln.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generate toy image data and tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Hyperparameter Tuning of a Small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Small Network Training on Next Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: torch.Size([1000, 19])\n",
      "Features Example: tensor([43180,  5924, 21651, 43102, 18571,  5080, 46038, 36541, 20136, 40897,\n",
      "        26704,  9526, 35570,  3553,   528, 27134, 22382, 37898,    83])\n",
      "Targets: torch.Size([1000, 19])\n",
      "Train: 700 Examples\n",
      "Test: 300 Examples\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "num_examples = 1000\n",
    "seq_len = 20\n",
    "data = torch.randint(0, vocab_size, (num_examples, 20))\n",
    "\n",
    "features = data[:, :-1]\n",
    "targets = data[:, 1:]\n",
    "\n",
    "print(f'Features: {features.shape}')\n",
    "print(f'Features Example: {features[0]}')\n",
    "print(f'Targets: {targets.shape}')\n",
    "\n",
    "dataset = TensorDataset(features, targets)\n",
    "\n",
    "train, test = random_split(dataset, lengths = [0.7, 0.3])\n",
    "\n",
    "print(f'Train: {len(train)} Examples')\n",
    "print(f'Test: {len(test)} Examples')\n",
    "\n",
    "train_loader = DataLoader(train, batch_size = 32, shuffle=True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim)\n",
    "    def forward(self, x):\n",
    "\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(-2, -1)\n",
    "        attention_weights = F.softmax(attention_scores/self.emb_dim**0.5, dim=-1)\n",
    "\n",
    "        context_vectors = attention_weights @ values\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.attn = SelfAttention(emb_dim)\n",
    "        self.ff = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        # print(x.shape)\n",
    "        x = self.attn(x)\n",
    "        # print(x.shape)\n",
    "        x = self.ff(x)\n",
    "        # print(x.shape)\n",
    "        x = x.permute(0,2,1)\n",
    "        # print(x.shape)\n",
    "        #x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 512\n",
    "s = SequenceNN(vocab_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0882, -0.0053, -0.0194,  ...,  0.0202,  0.0127,  0.0017],\n",
       "         [ 0.0556,  0.0846,  0.0866,  ...,  0.1088,  0.1312,  0.0834],\n",
       "         [ 0.1488,  0.1766,  0.1257,  ...,  0.1424,  0.1886,  0.1634],\n",
       "         ...,\n",
       "         [-0.0023,  0.0177,  0.0160,  ...,  0.0238,  0.0289,  0.0237],\n",
       "         [ 0.0548,  0.0844,  0.1051,  ...,  0.0895,  0.1299,  0.1313],\n",
       "         [ 0.0423,  0.0093,  0.0357,  ...,  0.0288,  0.0231,  0.0302]],\n",
       "\n",
       "        [[-0.0936, -0.0937, -0.0843,  ..., -0.1216, -0.0954, -0.0989],\n",
       "         [-0.0511, -0.0493, -0.0370,  ..., -0.0549, -0.0090, -0.0567],\n",
       "         [ 0.0091,  0.0444,  0.0353,  ...,  0.0014, -0.0042, -0.0239],\n",
       "         ...,\n",
       "         [ 0.1320,  0.1196,  0.1059,  ...,  0.1180,  0.0875,  0.1164],\n",
       "         [ 0.0028,  0.0112, -0.0011,  ..., -0.0261,  0.0041, -0.0033],\n",
       "         [-0.0055, -0.0247,  0.0156,  ..., -0.0053,  0.0030, -0.0215]],\n",
       "\n",
       "        [[-0.1127, -0.1297, -0.1166,  ..., -0.1081, -0.1183, -0.1325],\n",
       "         [-0.0587, -0.0460, -0.0248,  ..., -0.0437, -0.0223, -0.0598],\n",
       "         [ 0.0413,  0.0105,  0.0310,  ...,  0.0594,  0.0418,  0.0120],\n",
       "         ...,\n",
       "         [ 0.0770,  0.0236,  0.0789,  ...,  0.0390,  0.0150,  0.0212],\n",
       "         [ 0.0763,  0.0612,  0.1124,  ...,  0.0860,  0.0799,  0.0914],\n",
       "         [-0.0751, -0.0807, -0.0384,  ..., -0.0152, -0.0549, -0.0699]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1132, -0.0786, -0.1077,  ..., -0.1236, -0.0777, -0.0690],\n",
       "         [-0.0545, -0.0568, -0.0337,  ..., -0.0325, -0.0901, -0.0779],\n",
       "         [ 0.1112,  0.0611,  0.1464,  ...,  0.0905,  0.1149,  0.1123],\n",
       "         ...,\n",
       "         [-0.0566, -0.0453, -0.0696,  ..., -0.0521, -0.0226, -0.0608],\n",
       "         [ 0.0127, -0.0169, -0.0006,  ...,  0.0198, -0.0073,  0.0020],\n",
       "         [-0.0306,  0.0261,  0.0116,  ..., -0.0107,  0.0030,  0.0348]],\n",
       "\n",
       "        [[-0.0270, -0.0119, -0.0533,  ..., -0.0332, -0.0403, -0.0549],\n",
       "         [-0.0021, -0.0405,  0.0087,  ..., -0.0017, -0.0035, -0.0125],\n",
       "         [-0.0684, -0.0597, -0.0842,  ..., -0.0680, -0.0684, -0.0631],\n",
       "         ...,\n",
       "         [-0.0566, -0.0920, -0.0243,  ..., -0.0438, -0.0403, -0.0671],\n",
       "         [ 0.0776,  0.0926,  0.0882,  ...,  0.0493,  0.0860,  0.1070],\n",
       "         [-0.1059, -0.0982, -0.1209,  ..., -0.1095, -0.0981, -0.1104]],\n",
       "\n",
       "        [[-0.0445,  0.0063, -0.0184,  ..., -0.0447, -0.0395, -0.0256],\n",
       "         [-0.0747, -0.1124, -0.0302,  ..., -0.1111, -0.0748, -0.0953],\n",
       "         [ 0.0721,  0.0889,  0.0218,  ...,  0.0030,  0.0092,  0.0264],\n",
       "         ...,\n",
       "         [ 0.0402,  0.0013,  0.0641,  ...,  0.0345,  0.0489,  0.0600],\n",
       "         [ 0.0042,  0.0646, -0.0232,  ...,  0.0078, -0.0016, -0.0006],\n",
       "         [ 0.1256,  0.1478,  0.1367,  ...,  0.0671,  0.0728,  0.1911]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 5 Batch Loss: 10.825770378112793\n",
      "Epoch 1 Batch 10 Batch Loss: 10.82532787322998\n",
      "Epoch 1 Batch 15 Batch Loss: 10.81826114654541\n",
      "Epoch 1 Batch 20 Batch Loss: 10.818831443786621\n",
      "Epoch 1/4 Average Loss: 10.823479088869961\n",
      "Epoch 2 Batch 5 Batch Loss: 10.821355819702148\n",
      "Epoch 2 Batch 10 Batch Loss: 10.82321548461914\n",
      "Epoch 2 Batch 15 Batch Loss: 10.823677062988281\n",
      "Epoch 2 Batch 20 Batch Loss: 10.826794624328613\n",
      "Epoch 2/4 Average Loss: 10.823428240689365\n",
      "Epoch 3 Batch 5 Batch Loss: 10.824016571044922\n",
      "Epoch 3 Batch 10 Batch Loss: 10.822827339172363\n",
      "Epoch 3 Batch 15 Batch Loss: 10.820866584777832\n",
      "Epoch 3 Batch 20 Batch Loss: 10.828808784484863\n",
      "Epoch 3/4 Average Loss: 10.823449264873158\n",
      "Epoch 4 Batch 5 Batch Loss: 10.824984550476074\n",
      "Epoch 4 Batch 10 Batch Loss: 10.81948184967041\n",
      "Epoch 4 Batch 15 Batch Loss: 10.830227851867676\n",
      "Epoch 4 Batch 20 Batch Loss: 10.824949264526367\n",
      "Epoch 4/4 Average Loss: 10.82343647696755\n"
     ]
    }
   ],
   "source": [
    "cel = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(s.parameters(), lr = 0.0001)\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        outputs = s(inputs)\n",
    "        loss = cel(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss+=loss.item()\n",
    "        \n",
    "        # Report loss every 5 batches\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Batch {i + 1} Batch Loss: {loss.item()}\")\n",
    "\n",
    "    average_loss = training_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
