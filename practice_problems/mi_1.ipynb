{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Coding Questions (Python + PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to create a PyTorch tensor of shape (5, 3) filled with random floats between 0 and 1.\n",
    "Demonstrate how to perform element-wise addition, multiplication, and matrix multiplication between two tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Building and Training a Simple Model\n",
    "\n",
    "Implement a simple feedforward neural network in PyTorch that could, in theory, be used for sequence classification (e.g., classifying protein sequences into functional categories).\n",
    "Include a training loop with dummy data and explain each step of the process, including loss calculation and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.f1 = nn.Linear(embed_dim, n_hidden)\n",
    "        self.f2 = nn.Linear(n_hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        x=self.emb(x)\n",
    "        x = self.f1(x)\n",
    "        x = F.relu(x) #we choose ReLu because\n",
    "        x = self.f2(x)\n",
    "        return x\n",
    "\n",
    "#How do you choose the correct number of hidden units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleFeedForward(\n",
      "  (emb): Embedding(50000, 512)\n",
      "  (f1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (f2): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n",
      "Features: torch.Size([256, 6])\n",
      "Labels: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "#generate dummy data\n",
    "vocab_size = 50000\n",
    "embed_dim = 512\n",
    "num_examples = 256\n",
    "sequence_length = 6\n",
    "n_hidden = 256\n",
    "num_classes = 15\n",
    "\n",
    "sff = SimpleFeedForward(vocab_size, embed_dim, n_hidden, num_classes)\n",
    "print(sff)\n",
    "\n",
    "torch.manual_seed(89)\n",
    "features = torch.randint(0, vocab_size+1, (num_examples, sequence_length))\n",
    "\n",
    "labels = torch.randint(0, num_classes, (num_examples,))\n",
    "\n",
    "print(f'Features: {features.shape}')\n",
    "print(f'Labels: {labels.shape}')\n",
    "\n",
    "dataset = TensorDataset(features, labels)\n",
    "train_loader = DataLoader(dataset, batch_size = 32, shuffle=True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m sff(inputs)\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m, labels)\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(sff.parameters(), lr= 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "training_loss=0\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs, labels = batch\n",
    "    optimizer.zero_grad()\n",
    "    outputs = sff(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    training_loss+=loss.item()\n",
    "    print(training_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Sequential Data\n",
    "\n",
    "Given a sequence of amino acids (represented as a one-hot encoded tensor), write a PyTorch module that uses an LSTM layer to process the sequence and output a classification.\n",
    "Explain your choice of loss function and the significance of the LSTM's hidden state.\n",
    "Implementing a Custom Loss Function\n",
    "\n",
    "In the context of protein sequence generation, design a custom loss function in PyTorch that penalizes deviations from a target protein structure while encouraging novel therapeutics properties. Provide a high-level explanation of how you would implement and integrate this loss function into a training loop.\n",
    "Diffusion Models for Protein Generation\n",
    "\n",
    "Provide a brief overview of how you would implement a basic diffusion model in PyTorch for generating protein sequences. Discuss the key components of the model and how they work together to generate new sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate Dummy Data for the Next Token Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: torch.Size([1000, 19])\n",
      "Targets: torch.Size([1000, 19])\n",
      "Training Set: 700\n",
      "Testing Set: 300\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "batch_size = 32\n",
    "num_examples = 1000\n",
    "sequence_length = 20\n",
    "\n",
    "data = torch.randint(0, vocab_size+1, (num_examples, sequence_length))\n",
    "features = data[:, :-1]\n",
    "targets = data[:, 1:]\n",
    "\n",
    "print(f'Features: {features.shape}')\n",
    "print(f'Targets: {targets.shape}')\n",
    "\n",
    "dataset = TensorDataset(features, targets)\n",
    "train, test = random_split(dataset, lengths = [0.7, 0.3])\n",
    "\n",
    "print(f\"Training Set: {len(train)}\")\n",
    "print(f\"Testing Set: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generate a Simple Feed Forward Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFFBC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.f1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.f2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.f2(x) #no softmax because assuming Cross-Entropy Loss Later On\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFBC(\n",
      "  (f1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (f2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = 512\n",
    "sffbc = SFFBC(input_dim, 256)\n",
    "print(sffbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. #### Generate dummy data and train the simple net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: torch.Size([1000, 512])\n",
      "Labels: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "num_examples = 1000\n",
    "features = torch.randn(1000, 512)\n",
    "labels = torch.randint(0, 2, (num_examples,))\n",
    "\n",
    "print(f'Features: {features.shape}')\n",
    "print(f'Labels: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = random_split(TensorDataset(features, labels), lengths=[0.7,0.3])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch 0 Training Loss: 0.007162179797887802\n",
      "Epoch: 0 Batch 5 Training Loss: 0.12291115000844002\n",
      "Epoch: 0 Batch 10 Training Loss: 0.3158949481556192\n",
      "Epoch: 0 Batch 15 Training Loss: 0.5706950673134997\n",
      "Epoch: 0 Batch 20 Training Loss: 0.7860884377034381\n",
      "Epoch: 1 Batch 0 Training Loss: 0.8025870588840917\n",
      "Epoch: 1 Batch 5 Training Loss: 0.8381681521233986\n",
      "Epoch: 1 Batch 10 Training Loss: 1.054530896693177\n",
      "Epoch: 1 Batch 15 Training Loss: 1.5023427549560437\n",
      "Epoch: 1 Batch 20 Training Loss: 1.8411152269502054\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(sffbc.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "training_loss = 0\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    sffbc.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        outputs = sffbc(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "        if (i % 5 == 0):\n",
    "            print(f'Epoch: {epoch} Batch {i} Training Loss: {training_loss/5}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
